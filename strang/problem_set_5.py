# 5.2
'''
16)

Q:
Fn is the determinant of the 1, 1, -1 tridiagonal matrix of order n

F2 = [[1, 1], [-1, 1]]  F3 = [[1, 1, 0], [-1, 1, 1], [0, -1, 1]]  F4 = [[1, 1, 0, 0],[-1, 1, 1, 0],[0, -1, 1, 1],[0, 0, -1, 1]]

Expand in cofactors to show that Fn = Fn-1 + Fn-2. These determinants are fibonacci numbers 1, 2, 3, 5, 8, 13...
The sequence usually starts 1, 1, 2, 3 ... (with 2 1's) so our Fn is the usual Fn+1. (last clause?)

A:
The 1, 1 cofactor of an n x n matrix is Fn-1. So the 1, 1 cofactor of F4 is F3.

The 1, 2 cofactor of an n x n matrix has a 1 in the first column and Fn-2 in the rest. So the 1, 2 cofactor of F4 is [[1, 0, 0], [-1, 1, 1], [0, -1, 1]]

Multiply by (-1)1+2 and also (-1) from the 1, 2 entry to
find Fn = Fn-1 + Fn-2 (so these determinants are Fibonacci numbers).

'''


'''
32)

Q:
Cofactors of these 1, 3, 1 matrices give the recursion Sn = 3Sn-1 - Sn-2
This recursion produces every second Fibonacci number

S1 = [3]  S2 = [[3,1], [1,3]]  S3 = [[3, 1, 0], [1, 3, 1], [0, 1, 3]]

Show that Sn is the Fibonacci number F2n+2 by proving F2n+2 = 3F2n - F2n-2
Keep using Fibonaccis rule Fk = Fk-1 + Fk-2 starting with k = 2n + 2

'''

'''
33)

The symmetrical Pascal matrices have determinant 1.  If I subtract 1 from the n,n entry why does the determinant become 0?

det([
[1, 1, 1, 1],
[1, 2, 3, 4],
[1, 3, 6, 10],
[1, 4, 10, 20],
]) = 1 (known)

det([
[1, 1, 1, 1],
[1, 2, 3, 4],
[1, 3, 6, 10],
[1, 4, 10, 19],
]) = 0 (to explain)
'''


#5.3

'''
8)

Q:
Find the cofactors of A and multiply A(transpose(C)) to find det A:

A = [
    [1, 1, 4],
    [1, 2, 2],
    [1, 2, 5],
]

C = [
    [6, -3, 0],
    ...
    ...
]

A(transpose(C)) = ?

If you change that 4 to 100, why is det(A) unchanged?

A:

1[
    [2, 2],
    [2, 5],
] - 1[
    [1, 2],
    [1, 5],
] + 4[
    [1, 2],
    [1, 2],
] === 1*(2*5 - 2*2) - 1(1*5 - 1*2) + 4(1*2 - 1*2) === 6 - 3 + 0


the 1, 3 cofactor is 0, so the first multiple doesn't matter
'''

'''
27)

Polar coords satisfy x = rcosŒ∏ and y = rsinŒ∏.  Polar area is J dr dŒ∏.

J = [
    [Œ¥x/Œ¥r, Œ¥x/Œ¥Œ∏],
    [Œ¥y/Œ¥r, Œ¥y/Œ¥Œ∏]
] = [
    [cosŒ∏, -rsinŒ∏],
    [sinŒ∏, rcosŒ∏]
]

Note: this is Substituting for x=rcosŒ∏ && y=rsinŒ∏, so:
Œ¥x/Œ¥r === Œ¥rcosŒ∏/Œ¥r === cosŒ∏,
Œ¥x/Œ¥Œ∏ === Œ¥rcosŒ∏/Œ¥Œ∏ === -rsinŒ∏ -- the derivative of sin is cos and vice versa, This negative sign arises because as Œ∏ increases, cosŒ∏ is decreasing (moving downward on the graph), indicating a negative rate of change.
Œ¥y/Œ¥r === Œ¥rsinŒ∏/Œ¥r === sinŒ∏
Œ¥y/Œ¥Œ∏ === Œ¥rsinŒ∏/Œ¥Œ∏ === rcosŒ∏ -- Why there‚Äôs no sign change: The cosŒ∏ is positive in its derivative because sinŒ∏ is increasing as you increase Œ∏


The derivative of either x or y with respect to r simply gives a scaling factor, as there is no interaction with -sinŒ∏ (which only arises from derivatives with respect to theta. (r is scaling, theta is directional)


A:
the two columns are orthogonal. Their lengths are 1 and r.  thus J = r.

To compute the area of a region in polar coordinates, the integral is
Area=‚à´‚à´rdrdŒ∏.

'''

'''
28)

Q: Spherical coordinates œÅ (rho, radius), Œ∏ (theta, angle from z axis), …∏ (phi, angle from x axis) satisfy
x = œÅsin…∏cosŒ∏ and y = œÅsin…∏sinŒ∏ and z = œÅcos…∏.  Find the 3x3 matrix of partial derivatives.  Simplify its determinant to
J = p^2sin…∏.  Then dV in spherical coords is p^2sin…∏dpd…∏dŒ∏, the volume of an infinitesimal "coordinate box"

A:

J = [
    [Œ¥x/Œ¥p, Œ¥x/Œ¥…∏, Œ¥x/Œ¥Œ∏],
    [Œ¥y/Œ¥p, Œ¥y/Œ¥…∏, Œ¥y/Œ¥Œ∏],
    [Œ¥z/Œ¥p, Œ¥a/Œ¥…∏, Œ¥z/Œ¥Œ∏]
] = [
    [sin…∏cosŒ∏, psin…∏sinŒ∏, -pcos…∏cosŒ∏],
    [sin…∏sinŒ∏, psin…∏cosŒ∏, psinŒ∏cos…∏]
    [cos…∏, 0, -psin…∏]
] (Even without specific values for r,œï,Œ∏, this abstract expression describes the sensitivity of x to p, œï, and Œ∏)

x = psin…∏cosŒ∏
y = psin…∏sinŒ∏
z = pcos…∏

Œ¥x/Œ¥p = p(Œ¥/Œ¥p)sin…∏cosŒ∏ = sin…∏cosŒ∏
Œ¥x/Œ¥…∏ = p(Œ¥/Œ¥…∏)sin…∏cosŒ∏ = psin…∏sinŒ∏
Œ¥x/Œ¥Œ∏ = p(Œ¥/Œ¥Œ∏)sin…∏cosŒ∏ = -pcos…∏cosŒ∏

Œ¥y/Œ¥p = p(Œ¥/Œ¥p)sin…∏sinŒ∏ = sin…∏sinŒ∏
Œ¥y/Œ¥…∏ = p(Œ¥/Œ¥…∏)sin…∏sinŒ∏ = psin…∏cosŒ∏
Œ¥y/Œ¥Œ∏ = p(Œ¥/Œ¥Œ∏)sin…∏sinŒ∏ = pcos…∏sinŒ∏

Œ¥z/Œ¥p = p(Œ¥/Œ¥p)cos…∏ = cos…∏
Œ¥z/Œ¥…∏ = p(Œ¥/Œ¥…∏)cos…∏ = 0
Œ¥z/Œ¥Œ∏ = p(Œ¥/Œ¥Œ∏)cos…∏ = -psin…∏

sin…∏cosŒ∏(psin…∏cosŒ∏(-psin…∏) - 0*psin…∏cosŒ∏) - psin…∏sinŒ∏(sin…∏sinŒ∏(-psin…∏) - pcos…∏sinŒ∏cos…∏) + -pcos…∏cosŒ∏(sin…∏sinŒ∏*0 - cos…∏psin…∏cosŒ∏)
sin…∏cosŒ∏(-p^2sin…∏^2cosŒ∏) - psin…∏sinŒ∏(-psin…∏^2sinŒ∏ - pcos…∏^2sinŒ∏) + -pcos…∏cosŒ∏(-pcos…∏sin…∏cosŒ∏)

NOTE: the next line was based on a typo that has been corrected, disregard it in the context of the problem but I'm leaving it because
its nice to see how trigometric identities work.
-psin…∏^2sinŒ∏ - psin…∏cosŒ∏cos…∏ === -psin…∏(-sin…∏sinŒ∏ + cosŒ∏cos…∏) <-- trigometric identity -sin…∏sinŒ∏ + cosŒ∏cos…∏ === cos(Œ∏-…∏)

we can still factor out the middle, denser minor matrix result
-psin…∏^2sinŒ∏ - pcos…∏^2sinŒ∏ === -psinŒ∏(sin…∏^2 + cos…∏^2) <-- trigomatric identity sin…∏^2 + cos…∏^2 = 1

-p^2sin…∏^3cosŒ∏^2 - psin…∏sinŒ∏(-psinŒ∏) + p^2cos…∏^2sin…∏cosŒ∏^2
-p^2sin…∏^3cosŒ∏^2 + p^2sin…∏sinŒ∏^2 - p^2cos…∏^2sin…∏cosŒ∏^2

factor out p^2 and sin…∏

p^2sin…∏(-sin…∏^2cosŒ∏^2 + sinŒ∏^2 - cos…∏^2cosŒ∏^2)

now trigometric identity sin…∏^2 + cos…∏^2 = 1
-sin…∏^2cosŒ∏^2 - cos…∏^2cosŒ∏^2 === -cosŒ∏^2(sin…∏^2 + cos…∏^2) === -cosŒ∏^2(1)

p^2sin…∏(sinŒ∏^2 - cosŒ∏^2)

now trigometric identity sinŒ∏^2 - cosŒ∏^2 === -cos(2Œ∏)
p^2sin…∏(-cos(2Œ∏))


how does p^2sin…∏(-cos(2Œ∏)) simplify to -p^2sin…∏?

IF THE DETERMINANT IS A PART OF AN INTEGRAL OVER A FULL PERIOD OF Œ∏, -cos(2Œ∏) AVERAGES TO 0 AND THE RESULT WILL DEPEND ONLY ON r^2sin…∏

'''

'''
40)
Q:
suppose A is a 5x5 matrix.  Its entries in row 1 multiply determinants (cofactors) in rows 2-5 to
give the determinant.  Can you guess a "Jacobi formula" for det A using 2x2 determinants from
rows 1-2 times 3x3 determinants from rows 3-5?
Test your formula on the -1,2,-1 tridiagonal matrix that has a determinant = 6
M = [
    [2, -1, 0]
    [-1, 2, -1]
    [0, -1, 2]
]


A:
This is a recursive problem.

D23 === 2*2 - (-1)*(-1) === 4 - 1 === 3
D12 === 2*0 - (-1)*(-1) === 0 - 1 === -1

x11*D23 - x12*D12 === 2*3 - 0*(-1) === 6

So, to solve for a larger matrix:
Laplace expansion.  the steps are:
1. select k rows and k columns (doesn't this always reduce to 2 rows and 2 columns?)
2. compute the kxk determinant of the submatrix (so, yes, via cofactor expansion)
3. multiply the calculated determinant by the complementary matrix.

M = [
    [2, -1, 0, 0, 0]
    [-1, 2, -1, 0, 0]
    [0, -1, 2, -1, 0]
    [0, 0, -1, 2, -1]
    [0, 0, 0, -1, 2]
]

Since we're always focused on 2x2 matrices as the base case problem, for a 5x5 matrix there and 5 choose 2.  More generally n choose 2.

(5 choose 2 ) === 5!/2!(5-3)! === 10

So there are 10 possible combinations of rows and columns

pick (choose) 2 row indexes and 2 column indexes of the 5x5 matrix form 2x2 a submatrix to solve for.

‚àë(i,j i<j)(((-1)^i+j+1)*det(A1:2,i:j)*det(A3:5,complement)) . -- where for Ax:y,a:b x:y are rows and a:b are columns.

If you list all possible pairs of rows and columns for a 5x5 5x5 matrix:

Rows (1, 2) and Columns (1, 2)
Rows (1, 2) and Columns (1, 3)
Rows (1, 2) and Columns (1, 4)
Rows (1, 2) and Columns (1, 5)
Rows (1, 2) and Columns (2, 3)
Rows (1, 2) and Columns (2, 4)
Rows (1, 2) and Columns (2, 5)
Rows (1, 2) and Columns (3, 4)
Rows (1, 2) and Columns (3, 5)
Rows (1, 2) and Columns (4, 5)

this combinatorial approach holds for any _any square matrix_.

'''

'''
41)

Q:
The 2x2 matrix AB = (2 by 3)(3 by 2) has a "Cauchy-binet formula" for det(AB)

det(AB) = sum of 2x2 determinants to use from A and B

A) Guess which 2x2 determinants to use from A and B
B) Test your formula when the rows of A are 1,2,3 and 1,4,7 and B=transpose(A)

A:
A = [
    [1,2,3]
    [1,4,7]
]

B = [
    [1, 1]
    [2, 4]
    [3, 7]
]

AB = [
    [14, 30]
    [30, 66]
]


'''

'''
6.1

19)
A 3x3 matrix B is known to have eigenvalues 0,1,2. this is information enough to find these:
a) the rank of B
b) the determinant of transpose(B)B
b) the eigenvalues of transpose(B)B
b) the eigenvalues of (B^2 + I)^-1

so for eigenvalue x:
Bx = Œªx
Bx = 0x
Bx = 1x
Bx = 2x


det(A-ŒªI) = 0

a)
if 0 is an eigenvalue the matrix is singular.  Singular matrices are at most the rank of their dimensions minus 1, because all of the rows are not linearly independent
so rank is < 3

there are non-zero eigenvalues, so the rank is >0

if it were rank 1, there would only be one eigenvalue, corresponding to 1 independent vector

so x != 1, 0 < x < 3

x (rank) is 2

b)
transposition of a matrix doesn't affect the value of the determinant
The product of the eigenvalues of a matrix is equal to its determinant
det(transpose(B)B) === det(transpose(B))det(B) === det(B)^2 === (0*1*2)^2 === 0

c)


d)
In computational settings, (B^2 +I)^-1 is sometimes referred to as the inverse regularized matrix. (A shifted inverse or regularized inverse.)
This is due to its role in stabilizing computations when B^2 has eigenvalues close to zero.
In physics, appears in systems that involve damped responses or regularized inversions, especially in signal processing or control theory.
If B is diagonalizable, the eigenvalues of (B^2 +I)^-1 are given by a function

(B^2 + I) makes all of the elements positive with the sqaring, then adds the identity matrix so every element is +1, which ensures the matrix is not singular.

det(B) === ‚àè(Œª^i) -- multiplying the trace gives you the determinant.  If any element is 0, the determinant is 0.  So sqaring means -1 == 1, adding the identity shifts the trace +1.  So it's impossible for 0 to be a value in the trace.


applying a polynomial to a matrix, then transforming the eigenvector has the same result as applying the polynomial to an eigenvalue, then transforming (scaling) the eigenvector.
so p(B)x === (asubk*A^k + asubk-1*A^k-1 + asubk-2*A^k-2 ... + asub0)x === (asubk*Œª^k + asubk-1*Œª^k-1 + asubk-2*Œª^k-2 ... + asub0)x === p(Œª)x
NOTE: adding a constant to a matrix, like the last step in the polynomial, + asub0, adds the value across the trace, not every element in the matrix.  so the constant term is really asub0*ùüô

so if p(B) === (B^2 + ùüô)^-1 === 1/(B^2 + ùüô)
then
p(0) === 1/(0^2 + 1) === 1
p(1) === 1/(1^2 + 1) === 1/2
p(2) === 1/(2^2 + 1) === 1/5
'''

'''
6.1

29)
Q: find the eigenvalues of A, B, and C

A = [
    [1,2,3]
    [0,4,5]
    [0,0,6]
]

B = [
    [0,0,1]
    [0,2,0]
    [3,0,0]
]

C = [
    [2,2,2]
    [2,2,2]
    [2,2,2]
]

det(A - ŒªI)

det([
    [1 - Œª,2,3]
    [0,4 - Œª,5]
    [0,0,6 - Œª]
]) === 1-Œª((4-Œª)(6-Œª)) - 5*0 - 2*0 + 3*0 === 1-Œª(20-4Œª-6Œª+Œª^2) === 1-Œª(20-10Œª+Œª^2) === 20-20Œª-10Œª+10Œª^2+Œª^2-Œª^3 ===
-Œª^3+11Œª^2-30Œª+20 <-- wrong, disregard

The eigenvalues of a triangular matrix are the entries of the trace, so for A, Œª == {1,4,6}


Notes to help with C:
see Projection.md


A projection matrix is (v*transpose(v))/(transpose(v)*v)
so for a unit vector (1,1,1), the projection matrix is [
    [1/3, 1/3, 1/3]
    [1/3, 1/3, 1/3]
    [1/3, 1/3, 1/3]
]
this matrix projects any vector x onto the (column) space spanned by v

If the basis vector is scaled by 6, 6 * 1/3 === 2

so P = [
    [2,2,2]
    [2,2,2]
    [2,2,2]
] === 6*((1,1,1)transpose((1,1,1))/transpose((1,1,1))(1,1,1))

Pb === a((transpose(a)b)/(transpose(a)a)) === axÃÇ === p

det(P‚àíŒªI)=(2‚àíŒª)(‚àíŒª)(Œª+4)

since all 3 vectors are dependent, the rank is 1, there is is only 1 non-zero eigenvalue, 6.  Technically 6, 0, 0 are the eigenvalues for the 3x3 matrix.
'''


'''
6.2

6)
Q:

a
Describe all matrices S that diagonalize this Matrix A

A = [
    [4, 0]
    [1, 2]
]

b.
Describe all of the matrices that Diagonalize A^-1


A:
a.
S = [
    [0, 2]
    [1, 1]
]

or

S = [
    [2, 0]
    [1, 1]
]

b.

A^-1 = [
    [.25, 0]
    [-.125, .5]
]

The same as a.? -- yes

'''

'''
6.2

15)
Q: A^k approaches 0 as k -> infinity. if and only if every eigenvalue is what?  Which matrix has A^k -> 0?

A1 = [
    [.6, .9]
    [.4, .1]
]

A2 = [
    [.6, .9]
    [.1, .6]
]

A:
less than 1
The first has an eigenvalue of 1 so it approaches infinity
the second has all eigenvalues < 1 so it approaches 0

'''

'''
6.2

16)

Q:

a. find Œõ and S to diagonalize A1 in question 15.
b. what is the limit of Œõ^k as k -> infinity?
c. what is the limit of S(Œõ^k)S^1?
d. In the columns of this limiting matrix you see the what?

A:

a.
S = [
]

Œõ = [
]

b.


c.


d.
steady state



'''
